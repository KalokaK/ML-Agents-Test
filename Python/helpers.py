from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.base_env import DecisionSteps, TerminalSteps, BehaviorSpec, AgentId
import tensorflow as tf
from tensorflow import keras as K
import numpy as np
from datetime import datetime
import json
import time
import os
import re
from math import tau

from typing import Deque, Any, Dict, Tuple, List
from collections import deque
from dataclasses import dataclass
import parameters

parameters = parameters.parameters


@dataclass
class Trajectory:
    """
    Essentially just a storage class to neatly manage trajectories generated by agents.
    """
    observations: tf.Tensor
    actions: tf.Tensor
    rewards: tf.Tensor

    def as_tf(self):
        """
        converts the data contained inside the class to a tuple of tensors. Tensorflow is
        really not happy about if passed any type that it does not recognize (because of
        function tracing) and overriding its classes is a huge pain.
        :return:
        tuple with (observations, actions, rewards)
        """
        return self.observations, self.actions, self.rewards

    def info(self):
        """
        prints the shapes of the contained data
        """
        print(f'obs shape: {self.observations.shape}'
              f'act shape: {self.actions.shape}'
              f'rew shape: {self.rewards.shape}')


class WorkingTrajectory:
    def __init__(self):
        self.observations: List[np.ndarray] = []
        self.actions: List[np.ndarray] = []
        self.rewards: List[float] = []

    def clear(self):
        self.observations.clear()
        self.actions.clear()
        self.rewards.clear()

    def __len__(self):
        obs_len = len(self.observations)
        act_len = len(self.actions)
        rew_len = len(self.rewards)

        if (obs_len == act_len == rew_len) or (obs_len == act_len == rew_len - 1):
            return obs_len

        else:
            raise Exception('critical failure mismatched lenghts')

    def finalize(self):
        ret = self._finalize()
        self.clear()
        return ret

    def _finalize(self) -> Trajectory:
        """
        this may be made in to a @tf.function later on thus currently a sepperate method.
        its purpose is to convert the trajectory data to tensors
        :return:
        a Trajectory dataclass object of the current trajectory
        """
        obs = tf.convert_to_tensor(self.observations)
        act = tf.convert_to_tensor(self.actions, tf.float32)
        rew = tf.convert_to_tensor(self.rewards[1:], dtype=tf.float32)
        # print(f'obs shape: {obs.shape}'
        #       f'act shape: {act.shape}'
        #       f'rew shape: {rew.shape}'
        #       f'hex: {hex(id(self))}')  # DEBUG
        return Trajectory(obs, act, rew)


class Agent:
    """
    Mainly serves as an interface between the trajectories and the Behaviour Group. Also mirrors Unity's
    structure for clarity. Agents specific features may be added later on, for example observation visualization.
    """
    def __init__(self, id: AgentId):
        self.id: AgentId = id
        self.trajectory = WorkingTrajectory()

    def reset(self):
        """
        resets the agent age to its initialization state.
        """
        self.trajectory.clear()


class BehaviourGroup:
    """ TODO documentation
    The behaviour groups mirror the ML-Agents Brain concept. Each brain exhibits one behaviour pattern and may
    manage any number of agents. Each Behaviour Group thus trains its own policy by sampling from the interactions
    between its agents and the environment.
    """
    def __init__(self,
                 spec: BehaviorSpec,
                 spec_id,
                 no_step_decision_step: DecisionSteps,
                 scenario: str,
                 n_trajectories: int = parameters["N_TRAJECTORIES"],
                 sigma: float = parameters["SIGMA"],
                 gamma: float = parameters["GAMMA"],
                 parameters: Dict = parameters):
        """
        When a behaviour group is initialized it creates a Neural Network, A set of agents corresponding to its
        behaviour spec. It can be iterated over. if done so that yields an iterator containing the agents.
        This behaviour is deprecated and may be changed.

        :param spec:
        The Unity API BehaviourSpec object on which the behaviour Group should be modeled.

        :param spec_id:
        Serves as an Identifier, should mirror the Unity ML-Agents behaviour spec in the spec parameter.

        :param no_step_decision_step:
        an empty DecisionSteps object from the ML-Agents API. Because BehaviourSpecs do not give the number of agents
        or their ID's this is needed to instantiate the Agents which are controlled by the Behaviour Group.

        :param: scenario:
        the directory of the scenario data

        :param n_trajectories:
        An update step will be performed every n_trajectories. An Update will be performed every n_trajectories.
        Should be at least twice the number of agents of the environment since all unfinished training epochs are
        scrapped once the network trains! The larger the value the larger the ratio between usable and scrapped
        epochs. Too large of a value leads to extremely large updates and thus to instability.

        :param sigma:
        THIS PARAMETER IS DEPRECATED. it represents the standard deviation of the normal distribution from which
        actions are sampled. Since sigma is now trainable it is currently deprecated. May be used to set a minimum for
        sigma in future implementations.

        :param gamma:
        the discount factor for future rewards.
        """
        self.agents: Dict[AgentId, Agent] = {}  # a mapping from AgentId's to Agent objects
        self.vis_obs = False  # may be used if I add support for visual observations. currently DEPRECATED
        self.sigma = sigma  # this parameter currently has no use. a only mu net method needs to be implemented.
        self.gamma = gamma
        self.scenario = scenario
        self.debug = parameters["DEBUG"]
        self.done_counter = 0
        self.n_agents = len(no_step_decision_step)  # number of agents
        self.action_shape = spec.action_size  # action shape
        self.action_type = spec.action_type   # discrete or continuous
        self.observation_shapes = spec.observation_shapes  # obs shape
        self.n_trajectories = n_trajectories
        # acts as an efficient way of storing epoch data.
        self.trajectory_archive: Deque[Trajectory] = deque(maxlen=self.n_agents*n_trajectories)

        # initialize tensorflow stuff
        self.optimizer = tf.optimizers.Adam(lr=parameters["LEARN_RATE"])

        # resume training from a checkpoint.
        if parameters["LOAD_FROM_FILE"]:
            latest = 0.0
            for file in os.listdir(scenario + 'modelbackups'):
                if file.endswith('.json'):
                    current = float(file.split('_')[0])
                    latest = current if current > latest else latest

            with open(scenario + f'modelbackups/{latest}_training_params.json', 'r') as fp:
                data = json.load(fp)
            self.network = K.models.load_model(scenario + f'modelbackups/{data["time"]}_SIGMU_model.model')
            self.baseline_net = K.models.load_model(scenario + f'modelbackups/{data["time"]}_baseline_model.model')
            self.training_iteration = data["train_it"]
            self.id = data["behav_id"]
            self.log_writer = tf.summary.create_file_writer(
                scenario + f'logs/training_info/{str(datetime.fromtimestamp(latest))}')
        else:
            self.log_writer = tf.summary.create_file_writer(scenario + f'logs/training_info/{datetime.utcnow()}')
            self.network = self.get_network(self.observation_shapes, self.action_shape)
            self.baseline_net = self.get_baseline_network(self.observation_shapes)
            self.training_iteration = 0
            self.id = spec_id

        # prints out neat visualization of the networks.
        self.network.summary()
        self.baseline_net.summary()

        # initialize the agents.
        for id in no_step_decision_step.agent_id:
            self.agents[id] = Agent(id)

    def __len__(self) -> int:
        return len(self.agents)

    def __getitem__(self, agent_id: AgentId) -> Agent:
        return self.agents[agent_id]

    def __iter__(self):
        return iter(self.agents)

    def get_network(self, input_shapes, output_shape) -> K.Model:
        """
        get a Keras model through the functional API

        :param input_shapes:
        the shape of the observations. Do not include batch size as a dimension in input_shapes!

        :param output_shape:
        action shape. Do not include batch size as a dimension in output_shape!

        :return:
        A Keras Model adherent to the given parameters.
        """
        inputs = K.Input(int(np.sum(input_shapes)))
        dense_0 = K.layers.Dense(256, tf.nn.relu, name='den_0')(inputs)
        dense_1 = K.layers.Dense(128,  tf.nn.relu, name='den_1')(dense_0)
        dense_2 = K.layers.Dense(64,  tf.nn.relu, name='den_2')(dense_1)
        mu_0 = K.layers.Dense(32,  tf.nn.relu, name='mu_0')(dense_2)
        sig_0 = K.layers.Dense(32, tf.nn.relu, name='sig_0')(dense_2)
        mu_1 = K.layers.Dense(output_shape, name='mu_1',
                              bias_initializer=K.initializers.constant(parameters["BIAS_MU"]))(mu_0)
        sig_1 = K.layers.Dense(output_shape, name='sig_1',
                               bias_initializer=K.initializers.constant(parameters["BIAS_MU"]))(sig_0)
        sig_2 = tf.exp(sig_1, name='sig_2')
        return K.Model(inputs=inputs, outputs=[mu_1, sig_2])

    def get_baseline_network(self, input_shapes) -> K.Model:
        """
        gets a baseline network to estimate a state value function.

        :param input_shapes:
        the shape of the observations. Do not include batch size as a dimension in input_shapes!

        :return:
        A Keras Model adherent to the given parameters.
        """
        input_shapes = K.Input(int(np.sum(input_shapes)))
        dense_1 = K.layers.Dense(256, tf.nn.relu, kernel_initializer=K.initializers.he_normal())(input_shapes)
        dense_2 = K.layers.Dense(64, tf.nn.relu, kernel_initializer=K.initializers.he_normal())(dense_1)
        out = K.layers.Dense(1)(dense_2)
        return K.Model(inputs=input_shapes, outputs=out)

    def process_steps(self, decision_steps: DecisionSteps, terminal_steps: TerminalSteps) -> np.ndarray:
        """
        this takes in
        :param decision_steps:
        :param terminal_steps:
        :return:
        """
        if len(self.trajectory_archive) >= self.n_trajectories:
            self.train()
            if not parameters["DEBUG"]: self.save_network()

        actions = self.get_action(decision_steps.obs).numpy()

        for agent_id in decision_steps:
            decision_step = decision_steps[agent_id]
            obs = np.concatenate(decision_step.obs, 0)
            reward = decision_step.reward
            idx = decision_steps.agent_id_to_index[agent_id]
            action = actions[idx]

            # print(f'obs shape: {obs.shape}, act: {action}, reward: {reward }')  # DEBUG

            self.agents[agent_id].trajectory.observations.append(obs)
            self.agents[agent_id].trajectory.rewards.append(reward)
            self.agents[agent_id].trajectory.actions.append(action)

            # print(f'traj len of {self.agents[agent_id].trajectory.__len__()} for agent {agent_id}'
            #       f'hex traj is {hex(id(self.agents[agent_id].trajectory))}'
            #       f'hex traj obs is {hex(id(self.agents[agent_id].trajectory.observations))}')  # DEBUG

        for agent_id in terminal_steps:
            terminal_step = terminal_steps[agent_id]
            self.agents[agent_id].trajectory.rewards.append(terminal_step.reward)
            try:
                if self.agents[agent_id].trajectory.__len__() > 0:
                    self.trajectory_archive.append(self.agents[agent_id].trajectory.finalize())
                else:
                    self.agents[agent_id].trajectory.clear()
            except Exception as exc:
                self.agents[agent_id].trajectory.clear()
                continue

        return actions

    def train(self):
        for agent_id in self.agents:
            self.agents[agent_id].reset()
        print(f'network has trained!')
        self.training_iteration += 1

        logged_reward = 0
        logged_horizon = 0
        logged_delta = tf.zeros([1])
        logged_mu = tf.zeros([self.action_shape])
        logged_sig = tf.zeros([self.action_shape])
        logged_logpi = tf.zeros([self.action_shape])
        logged_loss = tf.zeros([self.action_shape])

        for trajectory in self.trajectory_archive:
            logged_reward += tf.reduce_sum(trajectory.rewards)
            logged_horizon += trajectory.rewards.shape[0]
            mu, sig, logpi, loss, delta = self._train_network(*trajectory.as_tf())
            print(mu, sig, logpi, loss, delta)
            logged_mu += mu
            logged_sig += sig
            logged_logpi += logpi
            logged_loss += loss
            logged_delta += delta

        archive_length = len(self.trajectory_archive)

        logged_reward /= archive_length
        logged_horizon /= archive_length
        logged_mu /= archive_length
        logged_sig /= archive_length
        logged_logpi /= archive_length
        logged_loss /= archive_length
        logged_delta /= archive_length

        print(
            logged_reward,
            logged_horizon,
            logged_mu,
            logged_sig,
            logged_logpi,
            logged_loss,
            logged_delta
        )

        self.trajectory_archive.clear()

        if not self.debug:
            with self.log_writer.as_default():
                tf.summary.scalar('reward', logged_reward, self.training_iteration)
                tf.summary.scalar('ep_length', logged_horizon, self.training_iteration)
                tf.summary.scalar('delta', logged_delta[0], self.training_iteration)
                tf.summary.histogram('mu', logged_mu, self.training_iteration)
                tf.summary.histogram('sig', logged_sig, self.training_iteration)
                tf.summary.histogram('logpi', logged_logpi, self.training_iteration)
                tf.summary.histogram('loss', logged_loss, self.training_iteration)
                tf.summary.flush()

    # DEBUG # @tf.function(experimental_relax_shapes=True)
    def _train_network(self, obs: tf.Tensor, act: tf.Tensor, rew: tf.Tensor):
        """

        :param obs:
        tensor with shape [episode_horizon, obs_shape]
        first time step at idx=0
        this does not contain timestep H! goes from t=0 -> H-1!

        :param act:
        tensor with shape [episode_horizon, act_shape]
        first time step at idx=0
        this does not contain timestep H! goes from t=0 -> H-1!

        :param rew:
        tensor with shape [episode_horizon, 1]
        first time step not included!
        this does not contain timestep 0! this tensor only goes from t=1 -> H!

        :return:
        tuple of mu, sig, logpi, loss_pi
        use those for logging
        """

        if self.debug:
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #
            tf.print('shapes obs, act, rew')  # DEBUG
            tf.print(obs.shape, act.shape, rew.shape)  # DEBUG
            tf.print(rew)  # DEBUG
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #

        # CREATING A TENSOR 'G' WHICH IS EQUAL TO SUM(k=idx-> Horizon) OF {rew[k] * y^{k-idx-1} }
        horizon = rew.shape[0]
        gamma = tf.ones([horizon, horizon], tf.float32) * self.gamma
        exponent = tf.range(horizon, dtype=tf.float32) - tf.expand_dims(tf.range(horizon, dtype=tf.float32), 1)
        """
        creates matrix of this size with shape [horizon, horizon]
        [[ 0,  1,  2]
         [-1,  0,  2]
         [-2, -1,  0]]
        """
        mask = tf.pow(gamma, exponent)  # raises each element in gamma to its corresponding exponent
        mask = tf.linalg.band_part(mask, 0, -1)  # multiplies the lower triangle of mask, excluding the diagonal, by 0
        G = rew * mask
        """
        G looks now like the following
        [[r0*y^0, r1*y^1, r2*y^2]      [[r0*1  , r1*y^1, r2*y^2]
         [r0*0  , r1*y^0, r2*y^1] <==>  [  0   , r1*1  , r2*y^1]  
         [r0*0  , r1*0  , r2*0^0]]      [  0   ,   0   , r2*1  ]] 
        """
        G = tf.reduce_sum(G, axis=1, keepdims=True)
        """
        G now is:
        [r0*1 + r1*y^1 + r2*y^2] ,   0 + r1*1 + r2*y^1 ,  0 + 0 + r2*1  ] 
         ===> G[idx] = SUM(k=idx-> Horizon) OF {rew[k] * y^{k-idx-1} }
         and G.shape is [Horizon, 1]
        """
        gamma_pow_t = tf.expand_dims(tf.pow(self.gamma, exponent[0]), 1)
        """
        gamma pow t is a vector where each value corresponds to gamma raised to the index of that value
        """
        with tf.GradientTape() as tape:
            baseline = self.baseline_net(obs)
            delta = tf.stop_gradient(G - baseline)

            """ DEBUG
            tf.print('delta.shape: ', end='')
            tf.print(delta.shape)
            """

            loss_base = - delta * baseline

            grads_base = tape.gradient(loss_base, self.baseline_net.trainable_variables)
            self.optimizer.apply_gradients(zip(grads_base, self.baseline_net.trainable_variables))

        with tf.GradientTape() as tape:
            mu, sig = self.network(obs)  # both have shape [horizon, action_size]
            # Obtain pdf of Gaussian distribution
            if self.debug: tf.print(mu, sig)

            pdf_value = tf.cast(tf.exp(-0.5 * ((act - mu) / sig) ** 2), tf.float32) * \
                        (1 / (sig * tf.sqrt(tau)))
            # Compute log probability, the + 1e-5 is to avoid log(0)
            log_probabilities: tf.Tensor = tf.math.log(pdf_value + 1e-5)
            # divide by action size to decrease learn rate proportionally
            # loss_pi is negative because gradient ascent = gradient descent of negative values
            loss_pi = - gamma_pow_t * delta * log_probabilities / self.action_shape

            grads_pi = tape.gradient(loss_pi, self.network.trainable_variables)
            grads_pi = [tf.clip_by_norm(g, parameters["CLIP"])
                        for g in grads_pi]
            self.optimizer.apply_gradients(zip(grads_pi, self.network.trainable_variables))

        if self.debug:
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #
            tf.print('shapes G, log_pi, loss_pi')  # DEBUG
            tf.print(G.shape, log_probabilities, loss_pi)  # DEBUG
            tf.print(G)  # DEBUG
            tf.print(delta)
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #

        return (tf.reduce_mean(mu, 0),
                tf.reduce_mean(sig, 0),
                tf.reduce_mean(log_probabilities, 0),
                tf.reduce_mean(loss_pi, 0),
                tf.reduce_mean(tf.abs(delta)))

    def save_network(self):
        now = time.time()
        utc_now = datetime.utcfromtimestamp(now)
        self.network.save(self.scenario + f'modelbackups/{utc_now}_SIGMU_model.model')
        self.baseline_net.save(self.scenario + f'modelbackups/{utc_now}_baseline_model.model')
        with open(self.scenario + f'modelbackups/{now}_training_params.json', 'w+') as fp:
            json.dump({
                'time': str(datetime.utcfromtimestamp(now)),
                'train_it': self.training_iteration,
                'behav_id': self.id
            }, fp)

    @tf.function
    def get_action(self, obs: List[np.ndarray]) -> tf.Tensor:
        obs = tf.concat(obs, 1)
        mu, sig = self.network(obs)
        action: tf.Tensor = tf.random.normal(mu.shape, mu, sig)

        if self.debug:
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #
            tf.print(action.shape)  # DEBUG
            tf.print(obs.shape)  # DEBUG
            tf.print('mu, sig: ', end='')  # DEBUG
            tf.print(mu, sig)  # DEBUG
            tf.print(mu.shape)  # DEBUG
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #
        return action

    def create_empty_action(self) -> np.ndarray:
        """
        Generates a numpy array corresponding to an empty action (all zeros)
        for a number of agents.
        """
        return np.zeros(self.action_shape, dtype=np.float32)

    def create_random_action(self) -> np.ndarray:
        """
        Generates a numpy array corresponding to a random action (either discrete
        or continuous) for a number of agents.
        """
        action = np.random.uniform(
            low=-1.0, high=1.0, size=self.action_shape
        ).astype(np.float32)
        return action

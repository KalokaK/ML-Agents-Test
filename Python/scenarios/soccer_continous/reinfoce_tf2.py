from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.base_env import DecisionSteps, TerminalSteps, BehaviorSpec, AgentId
import tensorflow as tf
from tensorflow import keras as K
import numpy as np
from datetime import datetime
import json
import time
import os
from math import tau

from typing import Deque, Any, Dict, Tuple, List
from collections import deque
from dataclasses import dataclass


parameters = {
    "EPISODES": 10000,
    "MONO_BEHAVIOUR": True,
    "N_TRAJECTORIES": 1,
    "SIGMA": 0.2,
    "GAMMA": 0.98,
    "LOAD_FROM_FILE": False,
}

'''
with open('Parameters.json') as j:
    tmp = json.load(j)
for key in tmp:
    parameters[key] = tmp[key]
del key, tmp'''

# Set up environment

env = UnityEnvironment(file_name='unity/SoccerTwos.x86_64', worker_id=1, seed=1, side_channels=[], no_graphics=False)
env.reset()
behaviour_names = list(env.behavior_specs)

optimizer = tf.optimizers.Adam(lr=0.00001)
log_writer = tf.summary.create_file_writer('logs/training_info/')


@dataclass
class Trajectory:
    """
    Essentially just a storage class to neatly manage trajectories generated by agents.
    """
    observations: tf.Tensor
    actions: tf.Tensor
    rewards: tf.Tensor

    def as_tf(self):
        """
        converts the data contained inside the class to a tuple of tensors. Tensorflow is
        really not happy about if passed any type that it does not recognize (because of
        function tracing) and overriding its classes is a huge pain.
        :return:
        tuple with (observations, actions, rewards)
        """
        return self.observations, self.actions, self.rewards

    def info(self):
        """
        prints the shapes of the contained data
        """
        print(f'obs shape: {self.observations.shape}'
              f'act shape: {self.actions.shape}'
              f'rew shape: {self.rewards.shape}')


class WorkingTrajectory:
    def __init__(self):
        self.observations: List[np.ndarray] = []
        self.actions: List[np.ndarray] = []
        self.rewards: List[float] = []

    def clear(self):
        self.observations.clear()
        self.actions.clear()
        self.rewards.clear()

    def __len__(self):
        obs_len = len(self.observations)
        act_len = len(self.actions)
        rew_len = len(self.rewards)

        if (obs_len == act_len == rew_len) or (obs_len == act_len == rew_len - 1):
            return obs_len

        else:
            raise Exception('critical failure mismatched lenghts')

    def finalize(self):
        ret = self._finalize()
        self.clear()
        return ret

    def _finalize(self) -> Trajectory:
        """
        this may be made in to a @tf.function later on thus currently a sepperate method.
        its purpose is to convert the trajectory data to tensors
        :return:
        a Trajectory dataclass object of the current trajectory
        """
        obs = tf.convert_to_tensor(self.observations)
        act = tf.convert_to_tensor(self.actions, tf.float32)
        rew = tf.convert_to_tensor(self.rewards[1:], dtype=tf.float32)
        print(f'obs shape: {obs.shape}'
              f'act shape: {act.shape}'
              f'rew shape: {rew.shape}'
              f'hex: {hex(id(self))}')  # DEBUG
        return Trajectory(obs, act, rew)


class Agent:
    def __init__(self, id: AgentId):
        self.id: AgentId = id
        self.episode = 0
        self.trajectory = WorkingTrajectory()
        self.done_flag = False

    def reset(self):
        self.episode = 0
        self.trajectory.clear()
        self.done_flag = False


class BehaviourGroup:
    """ TODO documentation

    """

    def __init__(self,
                 spec: BehaviorSpec,
                 spec_id,
                 no_step_decision_step: DecisionSteps,
                 mono_behaviour: bool = parameters["MONO_BEHAVIOUR"],
                 n_trajectories: int = parameters["N_TRAJECTORIES"],
                 sigma: float = parameters["SIGMA"],
                 gamma: float = parameters["GAMMA"]):
        self.agents: Dict[AgentId, Agent] = {}
        self.vis_obs = False
        self.sigma = sigma  # this parameter currently has no use. a only mu net method needs to be implemented.
        self.gamma = gamma
        self.done_counter = 0
        self.n_agents = no_step_decision_step.__len__()
        self.action_size = spec.action_size
        self.action_type = spec.action_type
        self.observation_shapes = spec.observation_shapes
        self.n_trajectories = n_trajectories
        self.trajectory_archive: Deque[Trajectory] = deque(maxlen=self.n_agents*n_trajectories)

        if parameters["LOAD_FROM_FILE"]:

            latest = 0.0
            for file in os.listdir('modelbackups'):
                if file.endswith('.json'):
                    current = float(file.split('_')[0])
                    latest = current if current > latest else latest
            with open(f'modelbackups/{latest}_training_params.json', 'r') as fp:
                data = json.load(fp)

            self.mu_network = K.models.load_model(f'modelbackups/{data["time"]}_mu_model.model')
            self.sig_network = K.models.load_model(f'modelbackups/{data["time"]}_sig_model.model')
            self.training_iteration = data["train_it"]
            self.id = data["behav_id"]
        else:
            self.mu_network = self.get_mu_network(self.observation_shapes, self.action_size)
            self.sig_network = self.get_sig_network(self.observation_shapes, self.action_size)
            self.training_iteration = 0
            self.id = spec_id

        self.mu_network.summary()
        self.sig_network.summary()

        for id in no_step_decision_step.agent_id:
            self.agents[id] = Agent(id)

    def __len__(self) -> int:
        return len(self.agents)

    def __getitem__(self, agent_id: AgentId) -> Agent:
        return self.agents[agent_id]

    def __iter__(self):
        return iter(self.agents)

    def get_mu_network(self, inputs, outputs):
        inputs = K.Input(int(np.sum(inputs)))
        dense_1 = K.layers.Dense(1024, tf.nn.relu)(inputs)
        dense_2 = K.layers.Dense(1024, tf.nn.relu)(dense_1)
        outputs = K.layers.Dense(outputs)(dense_2)
        return K.Model(inputs=inputs, outputs=outputs)

    def get_sig_network(self, inputs, outputs):
        inputs = K.Input(int(np.sum(inputs)))
        dense_1 = K.layers.Dense(256, tf.nn.relu)(inputs)
        dense_2 = K.layers.Dense(64, tf.nn.relu)(dense_1)
        outputs = tf.exp(K.layers.Dense(outputs)(dense_2))
        return K.Model(inputs=inputs, outputs=outputs)

    def process_steps(self, decision_steps: DecisionSteps, terminal_steps: TerminalSteps) -> np.ndarray:
        if self.done_counter == self.n_agents:
            self.train()
            self.save_network()

        actions = self.get_action(decision_steps.obs).numpy()

        for agent_id in decision_steps:
            if self.agents[agent_id].done_flag:
                continue
            decision_step = decision_steps[agent_id]
            obs = np.concatenate(decision_step.obs, 0)
            reward = decision_step.reward
            idx = decision_steps.agent_id_to_index[agent_id]
            action = actions[idx]

            # print(f'obs shape: {obs.shape}, act: {action}, reward: {reward }')  # DEBUG

            self.agents[agent_id].trajectory.observations.append(obs)
            self.agents[agent_id].trajectory.rewards.append(reward)
            self.agents[agent_id].trajectory.actions.append(action)

            # print(f'traj len of {self.agents[agent_id].trajectory.__len__()} for agent {agent_id}'
            #       f'hex traj is {hex(id(self.agents[agent_id].trajectory))}'
            #       f'hex traj obs is {hex(id(self.agents[agent_id].trajectory.observations))}')  # DEBUG

        for agent_id in terminal_steps:
            if self.agents[agent_id].done_flag:
                continue
            terminal_step = terminal_steps[agent_id]
            self.agents[agent_id].trajectory.rewards.append(terminal_step.reward)
            if self.agents[agent_id].trajectory.__len__() > 0:
                self.trajectory_archive.append(self.agents[agent_id].trajectory.finalize())

            self.agents[agent_id].episode += 1
            if self.agents[agent_id].episode >= self.n_trajectories:
                self.agents[agent_id].done_flag = True
                self.done_counter += 1
                '''
                DEBUG
                print(f'agent: {agent_id} in behaviour: {self.id} is done!')'''

        return actions

    def train(self):
        for agent_id in self.agents:
            self.agents[agent_id].reset()
        self.done_counter = 0
        print(f'network has trained!')
        self.training_iteration += 1

        logged_reward = 0

        for trajectory in self.trajectory_archive:

            trajectory.info()  # DEBUG
            logged_reward += tf.reduce_sum(trajectory.rewards)
            self._train_network(*trajectory.as_tf())

        logged_reward /= len(self.trajectory_archive)
        with log_writer.as_default():
            tf.summary.scalar('reward', logged_reward, self.training_iteration)

    # DEBUG # @tf.function(experimental_relax_shapes=True)
    def _train_network(self, obs: tf.Tensor, act: tf.Tensor, rew: tf.Tensor):
        """

        :param obs:
        tensor with shape [episode_horizon, obs_shape]
        first time step at idx=0
        this does not contain timestep H! goes from t=0 -> H-1!
        :param act:
        tensor with shape [episode_horizon, act_shape]
        first time step at idx=0
        this does not contain timestep H! goes from t=0 -> H-1!
        :param rew:
        tensor with shape [episode_horizon, 1]
        first time step not included!
        this does not contain timestep 0! this tensor only goes from t=1 -> H!
        :return:
        """

        # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #
        tf.print('shapes obs, act, rew')  # DEBUG
        tf.print(obs.shape, act.shape, rew.shape)  # DEBUG
        tf.print(rew)  # DEBUG
        # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #

        # CREATING A TENSOR 'G' WHICH IS EQUAL TO SUM(k=idx-> Horizon) OF {rew[k] * y^{k-idx-1} }
        horizon = rew.shape[0]
        gamma = tf.ones([horizon, horizon], tf.float32) * self.gamma
        exponent = tf.range(horizon, dtype=tf.float32) - tf.expand_dims(tf.range(horizon, dtype=tf.float32), 1)
        """
        creates matrix of this size with shape [horizon, horizon]
        [[ 0,  1,  2]
         [-1,  0,  2]
         [-2, -1,  0]]
        """
        mask = tf.pow(gamma, exponent)  # raises each element in gamma to its corresponding exponent
        mask = tf.linalg.band_part(mask, 0, -1)  # multiplies the lower triangle of mask, excluding the diagonal, by 0
        G = rew * mask
        """
        G looks now like the following
        [[r0*y^0, r1*y^1, r2*y^2]      [[r0*1  , r1*y^1, r2*y^2]
         [r0*0  , r1*y^0, r2*y^1] <==>  [  0   , r1*1  , r2*y^1]  
         [r0*0  , r1*0  , r2*0^0]]      [  0   ,   0   , r2*1  ]] 
        """
        G = tf.reduce_sum(G, axis=1, keepdims=True)
        """
        G now is:
        [r0*1 + r1*y^1 + r2*y^2] ,   0 + r1*1 + r2*y^1 ,  0 + 0 + r2*1  ] 
         ===> G[idx] = SUM(k=idx-> Horizon) OF {rew[k] * y^{k-idx-1} }
         and G.shape is [Horizon, 1]
        """
        with tf.GradientTape(persistent=True) as tape:
            mu = self.mu_network(obs)  # has shape [horizon, action_size]
            sig = self.sig_network(obs)  # has shape [horizon, action_size]
            # Obtain pdf of Gaussian distribution
            pdf_value = tf.cast(tf.exp(-0.5 * ((act - mu) / sig) ** 2), tf.float32) * \
                        (1 / (sig * tf.sqrt(tau)))
            # Compute log probability, the + 1e-5 is to avoid log(0)
            log_probabilities: tf.Tensor = tf.math.log(pdf_value + 1e-5)
            # Compute weighted loss
            loss = - G * log_probabilities / self.action_size # divide by action size to decrease learn proportionally

            grads_mu = tape.gradient(loss, self.mu_network.trainable_variables)
            optimizer.apply_gradients(zip(grads_mu, self.mu_network.trainable_variables))

            grads_sig = tape.gradient(loss, self.sig_network.trainable_variables)
            optimizer.apply_gradients(zip(grads_sig, self.sig_network.trainable_variables))

            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #
            tf.print('shapes G, log_pi, loss')  # DEBUG
            tf.print(G.shape, log_probabilities.shape, loss.shape)  # DEBUG
            tf.print(G)  # DEBUG
            # FOR DEBUGGING ONLY DISABLE WHEN TRAINING #

    def save_network(self):
        now = time.time()
        self.mu_network.save(f'modelbackups/{datetime.utcfromtimestamp(now)}_mu_model.model')
        with open(f'modelbackups/{now}_training_params.json', 'w+') as fp:
            json.dump({
                'time': str(datetime.utcfromtimestamp(now)),
                'train_it': self.training_iteration,
                'behav_id': self.id
            }, fp)

    @tf.function
    def get_action(self, obs: List[np.ndarray]) -> tf.Tensor:
        obs = tf.concat(obs, 1)
        # tf.print(obs.shape)  # DEBUG
        mu = self.mu_network(obs)
        sig = self.sig_network(obs)
        # tf.print(mu.shape)  # DEBUG
        action: tf.Tensor = tf.random.normal(mu.shape, mu, sig)
        # tf.print(action.shape)  # DEBUG

        return action

    def create_empty_action(self) -> np.ndarray:
        """
        Generates a numpy array corresponding to an empty action (all zeros)
        for a number of agents.
        """
        return np.zeros(self.action_size, dtype=np.float32)

    def create_random_action(self) -> np.ndarray:
        """
        Generates a numpy array corresponding to a random action (either discrete
        or continuous) for a number of agents.
        """
        action = np.random.uniform(
            low=-1.0, high=1.0, size=self.action_size
        ).astype(np.float32)
        return action


behaviour_groups: Dict[str, BehaviourGroup] = {}

for key in list(env.behavior_specs):
    spec = env.behavior_specs[key]
    decision_steps, terminal_steps = env.get_steps(key)
    behaviour_groups[key] = BehaviourGroup(spec, key, decision_steps, mono_behaviour=parameters['MONO_BEHAVIOUR'])
    print('behaviour name: ', key)
    print('number of observations: ', len(spec.observation_shapes))
    behaviour_groups[key].vis_obs = vis_obs = any(len(shape) == 3 for shape in spec.observation_shapes)
    print('is visual: ', vis_obs, '\n')
    print('continuous action space: ', spec.is_action_continuous(),
          ', discrete action space: ', spec.is_action_discrete())

env.reset()
step = 0
while True:
    time_step = time.time_ns()
    for group in behaviour_groups: # group is the spec ID
        t0 = time.time_ns()
        decision_steps, terminal_steps = env.get_steps(group)
        t1 = time.time_ns()
        actions = behaviour_groups[group].process_steps(decision_steps, terminal_steps)
        t2 = time.time_ns()
        env.set_actions(group, actions)
        t3 = time.time_ns()
        if step%100 == 0:
            print(f'time for group {group} in ms, total: {(t3-t0)/10e6}, get steps: {(t1-t0)/10e6}, get actions: {(t2-t1)/10e6}, set actions: {(t3-t2)/10e6}')
    time_bef_step = time.time_ns()
    env.step()
    time_post_step = time.time_ns()
    total = time_post_step - time_step
    for_python = time_bef_step - time_step
    for_unity = time_post_step - time_bef_step
    if step%100 == 0:
        print(f'step took {total/10e6} ms, for python: {for_python/10e6}, for unity: {for_unity/10e6}, percent python: {100*for_python/total}, percent unity: {100*for_unity/total} \n')
    step += 1

env.close()

